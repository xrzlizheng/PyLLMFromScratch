{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "llama4-intro",
      "metadata": {
        "id": "llama4-intro"
      },
      "source": [
        "## End-to-End Llama 4 Mixture-of-Experts (MoE) Text Model Implementation (Inline)\n",
        "\n",
        "### Introduction: Exploring Llama 4's MoE Architecture\n",
        "\n",
        "This notebook provides a detailed, step-by-step implementation of a simplified text generation model based on the concepts found in advanced architectures like Llama 4, with a specific focus on the **Mixture-of-Experts (MoE)** component. We aim to understand how MoE layers operate within a Transformer framework.\n",
        "\n",
        "**Key Concepts from Llama 4 Implemented Here:**\n",
        "\n",
        "1.  **Transformer Core:** The fundamental building blocks remain the Transformer architecture (Self-Attention, Feed-Forward layers).\n",
        "2.  **RMSNorm:** Root Mean Square Normalization is used instead of standard Layer Normalization.\n",
        "3.  **Rotary Positional Embeddings (RoPE):** To inject sequence order information directly into the attention mechanism.\n",
        "4.  **Mixture-of-Experts (MoE):** Replacing some standard Feed-Forward layers with MoE blocks. Each MoE block contains:\n",
        "    *   Multiple smaller \"expert\" networks (MLPs).\n",
        "    *   A \"router\" network that learns to direct each input token to a small subset (Top-K) of these experts.\n",
        "    *   A mechanism to combine the outputs from the selected experts.\n",
        "    *   Often includes a \"shared\" expert (a standard MLP) whose output is combined with the MoE output.\n",
        "5.  **Gated MLP:** The expert MLPs (and the shared expert) use a gated activation (like SiLU) similar to standard Llama models.\n",
        "\n",
        "**Simplifications for this Inline Implementation:**\n",
        "\n",
        "*   **Attention:** We will implement standard Multi-Head Attention (MHA) for simplicity, although Llama models often use optimizations like Grouped Query Attention (GQA).\n",
        "*   **Scale:** We will use significantly smaller dimensions (`d_model`, `n_layers`, number of experts) and a smaller dataset (character-level tokenization on a small text corpus) for feasibility.\n",
        "*   **Optimizations:** Features like L2 Norm on queries/keys, temperature tuning, and chunked attention are omitted to focus on the core MoE structure.\n",
        "\n",
        "**Style:**\n",
        "\n",
        "Following the requested style, this implementation uses:\n",
        "*   Small, sequential code blocks.\n",
        "*   Detailed theoretical explanations before each block.\n",
        "*   Inline execution (minimal use of functions/classes).\n",
        "*   `print` statements to show intermediate shapes and values.\n",
        "\n",
        "Let's begin by setting up the environment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-setup-theory",
      "metadata": {
        "id": "llama4-setup-theory"
      },
      "source": [
        "### Step 0: Setup - Libraries, Seed, and Device\n",
        "\n",
        "**Goal:** Prepare the environment by importing necessary libraries (PyTorch for tensor operations and neural networks, math for calculations) and setting up for reproducibility and hardware selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "llama4-setup-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-setup-code",
        "outputId": "9ae3edcd-c69e-47a9-983a-65f2ecb4957d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "Using device: cuda\n",
            "Libraries imported and device configured.\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import os\n",
        "import collections # For BPE-like processing if extended\n",
        "import re          # For initial splitting\n",
        "\n",
        "# For reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "\n",
        "# --- Device Configuration ---\n",
        "# Theory: Set the device (GPU 'cuda' if available, else CPU) for tensor operations.\n",
        "# This ensures models and data are processed efficiently on available hardware.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"Libraries imported and device configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-setup-output",
      "metadata": {
        "id": "llama4-setup-output"
      },
      "source": [
        "**Output Explanation:** The code imports PyTorch and other standard libraries. It sets a random seed for reproducibility and checks if a CUDA-enabled GPU is available, setting the `device` variable accordingly. The output confirms the PyTorch version and the device being used (e.g., 'cuda' or 'cpu')."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-corpus-theory",
      "metadata": {
        "id": "llama4-corpus-theory"
      },
      "source": [
        "### Step 1: Corpus Definition and Character-Level Tokenization\n",
        "\n",
        "**Goal:** Define the text data we will use for training and create a simple character-level tokenizer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-corpus-define-theory",
      "metadata": {
        "id": "llama4-corpus-define-theory"
      },
      "source": [
        "#### Step 1.1: Define the Training Corpus\n",
        "\n",
        "**Theory:** We need some text data to train our language model. For simplicity and demonstration, we'll use a small excerpt from Lewis Carroll's \"Alice's Adventures in Wonderland\". A real model would be trained on vastly larger datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "llama4-corpus-define-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-corpus-define-code",
        "outputId": "e6585d68-1f63-4270-a81f-230e20ba0120"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training corpus defined (length: 593 characters).\n"
          ]
        }
      ],
      "source": [
        "# Define the raw text corpus for training\n",
        "corpus_raw = \"\"\"\n",
        "Alice was beginning to get very tired of sitting by her sister on the\n",
        "bank, and of having nothing to do: once or twice she had peeped into the\n",
        "book her sister was reading, but it had no pictures or conversations in\n",
        "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
        "conversation?'\n",
        "So she was considering in her own mind (as well as she could, for the\n",
        "hot day made her feel very sleepy and stupid), whether the pleasure\n",
        "of making a daisy-chain would be worth the trouble of getting up and\n",
        "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
        "close by her.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"Training corpus defined (length: {len(corpus_raw)} characters).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-corpus-define-output",
      "metadata": {
        "id": "llama4-corpus-define-output"
      },
      "source": [
        "**Output Explanation:** This defines the `corpus_raw` string variable containing our sample text and prints its total character length."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-corpus-tokenize-theory",
      "metadata": {
        "id": "llama4-corpus-tokenize-theory"
      },
      "source": [
        "#### Step 1.2: Character-Level Tokenization\n",
        "\n",
        "**Theory:** We'll create the simplest form of tokenizer. We find all unique characters present in the corpus. Then, we create two dictionaries: `char_to_int` maps each unique character to a unique integer ID, and `int_to_char` provides the reverse mapping. The total number of unique characters determines our vocabulary size (`vocab_size`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "llama4-corpus-tokenize-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-corpus-tokenize-code",
        "outputId": "87179485-3f45-49c3-c206-4fac4b6f2405"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created character vocabulary of size: 36\n",
            "Vocabulary: \n",
            " '(),-.:?ARSWabcdefghiklmnoprstuvwy\n"
          ]
        }
      ],
      "source": [
        "# Find all unique characters in the raw corpus\n",
        "chars = sorted(list(set(corpus_raw)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# Create character-to-integer mapping (encoding)\n",
        "char_to_int = { ch:i for i,ch in enumerate(chars) }\n",
        "\n",
        "# Create integer-to-character mapping (decoding)\n",
        "int_to_char = { i:ch for i,ch in enumerate(chars) }\n",
        "\n",
        "print(f\"Created character vocabulary of size: {vocab_size}\")\n",
        "print(f\"Vocabulary: {''.join(chars)}\")\n",
        "# Optional: Print mappings\n",
        "# print(f\"Char-to-Int mapping sample: {{k: char_to_int[k] for k in list(char_to_int)[:5]}}\")\n",
        "# print(f\"Int-to-Char mapping sample: {{k: int_to_char[k] for k in list(int_to_char)[:5]}}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-corpus-tokenize-output",
      "metadata": {
        "id": "llama4-corpus-tokenize-output"
      },
      "source": [
        "**Output Explanation:** The code identifies all unique characters, calculates the `vocab_size`, and creates the forward (`char_to_int`) and reverse (`int_to_char`) lookup tables. The output shows the vocabulary size and the characters included."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-corpus-encode-theory",
      "metadata": {
        "id": "llama4-corpus-encode-theory"
      },
      "source": [
        "#### Step 1.3: Encode the Corpus\n",
        "\n",
        "**Theory:** Convert the entire raw text corpus into a sequence of integer token IDs using the `char_to_int` mapping. This numerical sequence is the actual input the model will process. We store this as a PyTorch tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "llama4-corpus-encode-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-corpus-encode-code",
        "outputId": "e6b05a53-3c40-467d-b9a9-fb107d7115cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Encoded corpus into a tensor of shape: torch.Size([593])\n"
          ]
        }
      ],
      "source": [
        "# Encode the entire corpus into a list of integer IDs\n",
        "encoded_corpus = [char_to_int[ch] for ch in corpus_raw]\n",
        "\n",
        "# Convert the list into a PyTorch tensor\n",
        "full_data_sequence = torch.tensor(encoded_corpus, dtype=torch.long, device=device)\n",
        "\n",
        "print(f\"Encoded corpus into a tensor of shape: {full_data_sequence.shape}\")\n",
        "# Optional: Display first few encoded IDs\n",
        "# print(f\"First 50 encoded token IDs: {full_data_sequence[:50].tolist()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-corpus-encode-output",
      "metadata": {
        "id": "llama4-corpus-encode-output"
      },
      "source": [
        "**Output Explanation:** The raw corpus string is converted into a PyTorch tensor where each character is replaced by its integer ID. The output shows the shape of this tensor (which should be `[corpus_length]`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-hparams-theory",
      "metadata": {
        "id": "llama4-hparams-theory"
      },
      "source": [
        "### Step 2: Define Hyperparameters\n",
        "\n",
        "**Goal:** Set the configuration values that control the size, structure, and training of our Llama 4-like MoE model. We reference the configuration from the provided Llama 4 script but scale down the dimensions significantly for this demonstration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "llama4-hparams-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-hparams-code",
        "outputId": "5287c2ca-b84c-4da6-b8be-7ce3185ebc04"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Hyperparameters Defined ---\n",
            "Vocabulary Size (vocab_size): 36\n",
            "Embedding Dimension (d_model): 128\n",
            "Number of Layers (n_layers): 4\n",
            "Number of Attention Heads (n_heads): 4\n",
            "Dimension per Head (d_k): 32\n",
            "Max Sequence Length (block_size): 64\n",
            "RMSNorm Epsilon (rms_norm_eps): 1e-05\n",
            "RoPE Theta (rope_theta): 10000.0\n",
            "--- MoE Specific ---\n",
            "Number of Local Experts (num_local_experts): 4\n",
            "Experts per Token (num_experts_per_tok): 2\n",
            "Expert Intermediate Size (expert_dim): 256\n",
            "Shared MLP Intermediate Size (shared_expert_dim): 256\n",
            "--- Training Specific ---\n",
            "Learning Rate: 0.0005\n",
            "Batch Size: 16\n",
            "Epochs: 3000\n"
          ]
        }
      ],
      "source": [
        "# --- Model Architecture Hyperparameters ---\n",
        "# vocab_size is already determined from the data\n",
        "d_model = 128         # Embedding dimension (reduced significantly)\n",
        "n_layers = 4          # Number of Transformer blocks (reduced)\n",
        "n_heads = 4           # Number of attention heads\n",
        "block_size = 64       # Maximum context length (sequence length)\n",
        "rms_norm_eps = 1e-5   # Epsilon for RMSNorm stability\n",
        "rope_theta = 10000.0  # Theta parameter for RoPE (reduced from Llama 4's 500k)\n",
        "\n",
        "# --- MoE Specific Hyperparameters ---\n",
        "num_local_experts = 4      # Number of experts per MoE layer (reduced from 16)\n",
        "num_experts_per_tok = 2   # Number of experts to route each token to (Top-K, reduced from 4?)\n",
        "intermediate_size_expert = d_model * 2  # Hidden dimension within each expert MLP (scaled down)\n",
        "intermediate_size_shared = d_model * 2  # Hidden dimension within the shared MLP (scaled down)\n",
        "\n",
        "# --- Attention Hyperparameters ---\n",
        "# d_k (dimension per head) will be derived from d_model and n_heads\n",
        "\n",
        "# --- Training Hyperparameters ---\n",
        "learning_rate = 5e-4  # Learning rate\n",
        "batch_size = 16       # Number of sequences processed in parallel\n",
        "epochs = 3000         # Number of training iterations (adjust as needed)\n",
        "eval_interval = 300  # How often to print loss\n",
        "\n",
        "# --- Derived Hyperparameters ---\n",
        "assert d_model % n_heads == 0, \"d_model must be divisible by n_heads\"\n",
        "d_k = d_model // n_heads # Dimension of keys/queries/values per head\n",
        "expert_dim = intermediate_size_expert # Alias for clarity\n",
        "shared_expert_dim = intermediate_size_shared # Alias for clarity\n",
        "\n",
        "print(\"--- Hyperparameters Defined ---\")\n",
        "print(f\"Vocabulary Size (vocab_size): {vocab_size}\")\n",
        "print(f\"Embedding Dimension (d_model): {d_model}\")\n",
        "print(f\"Number of Layers (n_layers): {n_layers}\")\n",
        "print(f\"Number of Attention Heads (n_heads): {n_heads}\")\n",
        "print(f\"Dimension per Head (d_k): {d_k}\")\n",
        "print(f\"Max Sequence Length (block_size): {block_size}\")\n",
        "print(f\"RMSNorm Epsilon (rms_norm_eps): {rms_norm_eps}\")\n",
        "print(f\"RoPE Theta (rope_theta): {rope_theta}\")\n",
        "print(\"--- MoE Specific ---\")\n",
        "print(f\"Number of Local Experts (num_local_experts): {num_local_experts}\")\n",
        "print(f\"Experts per Token (num_experts_per_tok): {num_experts_per_tok}\")\n",
        "print(f\"Expert Intermediate Size (expert_dim): {expert_dim}\")\n",
        "print(f\"Shared MLP Intermediate Size (shared_expert_dim): {shared_expert_dim}\")\n",
        "print(\"--- Training Specific ---\")\n",
        "print(f\"Learning Rate: {learning_rate}\")\n",
        "print(f\"Batch Size: {batch_size}\")\n",
        "print(f\"Epochs: {epochs}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-hparams-output",
      "metadata": {
        "id": "llama4-hparams-output"
      },
      "source": [
        "**Output Explanation:** This block defines and prints all the crucial hyperparameters governing the model's size (like `d_model`, `n_layers`, `n_heads`, `num_local_experts`), its behavior (like `rope_theta`, `rms_norm_eps`), and the training process (like `learning_rate`, `batch_size`, `epochs`). Derived values like `d_k` are also calculated and displayed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-dataprep-theory",
      "metadata": {
        "id": "llama4-dataprep-theory"
      },
      "source": [
        "### Step 3: Data Preparation for Training\n",
        "\n",
        "**Goal:** Structure the encoded data (`full_data_sequence`) into input (`x`) and target (`y`) pairs suitable for training the next-token prediction task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-dataprep-pairs-theory",
      "metadata": {
        "id": "llama4-dataprep-pairs-theory"
      },
      "source": [
        "#### Step 3.1: Create Input (x) and Target (y) Pairs\n",
        "\n",
        "**Theory:** The model learns to predict the next token based on the preceding ones. We create overlapping sequences of length `block_size` from our encoded corpus. For an input sequence `x` starting at index `i`, the corresponding target sequence `y` starts at index `i+1`. Each token in `y` is the target prediction for the corresponding token in `x`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "llama4-dataprep-pairs-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-dataprep-pairs-code",
        "outputId": "9d3ebe5f-1035-4538-f774-043e45fc54a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 529 overlapping input/target sequence pairs.\n",
            "Shape of train_x: torch.Size([529, 64])\n",
            "Shape of train_y: torch.Size([529, 64])\n"
          ]
        }
      ],
      "source": [
        "# Create lists to hold all possible input (x) and target (y) sequences\n",
        "all_x = []\n",
        "all_y = []\n",
        "\n",
        "# Iterate through the encoded corpus tensor to extract overlapping sequences\n",
        "num_total_tokens = len(full_data_sequence)\n",
        "for i in range(num_total_tokens - block_size):\n",
        "    # Extract the input sequence chunk\n",
        "    x_chunk = full_data_sequence[i : i + block_size]\n",
        "    # Extract the target sequence chunk (shifted one position right)\n",
        "    y_chunk = full_data_sequence[i + 1 : i + block_size + 1]\n",
        "    all_x.append(x_chunk)\n",
        "    all_y.append(y_chunk)\n",
        "\n",
        "# Stack the lists of tensors into single large tensors\n",
        "train_x = torch.stack(all_x)\n",
        "train_y = torch.stack(all_y)\n",
        "\n",
        "num_sequences_available = train_x.shape[0]\n",
        "print(f\"Created {num_sequences_available} overlapping input/target sequence pairs.\")\n",
        "print(f\"Shape of train_x: {train_x.shape}\") # Should be (num_sequences, block_size)\n",
        "print(f\"Shape of train_y: {train_y.shape}\") # Should be (num_sequences, block_size)\n",
        "\n",
        "# Optional: Verify device\n",
        "# print(f\"train_x is on device: {train_x.device}\") # May still be on CPU, move in batching"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-dataprep-pairs-output",
      "metadata": {
        "id": "llama4-dataprep-pairs-output"
      },
      "source": [
        "**Output Explanation:** The code extracts all possible sequences of length `block_size` and their corresponding target sequences from the encoded text. It then stacks these into two tensors, `train_x` and `train_y`. The output shows the number of sequence pairs created and their shapes."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-dataprep-batching-theory",
      "metadata": {
        "id": "llama4-dataprep-batching-theory"
      },
      "source": [
        "#### Step 3.2: Batching Strategy (Random Sampling)\n",
        "\n",
        "**Theory:** For training, we process data in batches. Instead of a formal DataLoader, we'll randomly sample `batch_size` indices from our available sequences (`0` to `num_sequences_available - 1`) in each training step. We then retrieve the corresponding sequences from `train_x` and `train_y` and move them to the correct `device`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "llama4-dataprep-batching-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-dataprep-batching-code",
        "outputId": "34847a98-2638-42ff-9afb-ff73f05bed0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data ready for training. Will sample batches of size 16 randomly.\n",
            "Batches will be moved to device during the training loop.\n"
          ]
        }
      ],
      "source": [
        "# Check if we have enough sequences for the desired batch size\n",
        "if num_sequences_available < batch_size:\n",
        "    print(f\"Warning: Number of sequences ({num_sequences_available}) is less than batch size ({batch_size}). Adjusting batch size.\")\n",
        "    batch_size = num_sequences_available\n",
        "\n",
        "print(f\"Data ready for training. Will sample batches of size {batch_size} randomly.\")\n",
        "print(\"Batches will be moved to device during the training loop.\")\n",
        "# Example of how a batch would be selected in the loop:\n",
        "# indices = torch.randint(0, num_sequences_available, (batch_size,))\n",
        "# xb = train_x[indices].to(device)\n",
        "# yb = train_y[indices].to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-dataprep-batching-output",
      "metadata": {
        "id": "llama4-dataprep-batching-output"
      },
      "source": [
        "**Output Explanation:** Confirms the batching strategy. It checks if the batch size is feasible given the number of sequences and explains that random sampling will be used within the training loop to create batches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-theory",
      "metadata": {
        "id": "llama4-init-theory"
      },
      "source": [
        "### Step 4: Model Component Initialization (Inline)\n",
        "\n",
        "**Goal:** Initialize the learnable parameters and fixed components for our Llama 4-like MoE model. We will store components for each layer in lists for use within the training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-embed-theory",
      "metadata": {
        "id": "llama4-init-embed-theory"
      },
      "source": [
        "#### Step 4.1: Token Embedding Layer\n",
        "\n",
        "**Theory:** This layer maps discrete token IDs (our character IDs) to dense vectors of dimension `d_model`. It's essentially a lookup table where each row corresponds to a token in the vocabulary. Input shape `(B, T)` -> Output shape `(B, T, C)`, where `B` is batch size, `T` is sequence length (`block_size`), and `C` is embedding dimension (`d_model`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "llama4-init-embed-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-init-embed-code",
        "outputId": "b724c999-71cc-49c2-cadd-37b9b467853d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized Token Embedding Layer:\n",
            "  Input Vocab Size: 36\n",
            "  Output Embedding Dim (d_model): 128\n",
            "  Weight shape: torch.Size([36, 128])\n",
            "  Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Initialize the token embedding table\n",
        "token_embedding_table = nn.Embedding(vocab_size, d_model).to(device)\n",
        "\n",
        "print(f\"Initialized Token Embedding Layer:\")\n",
        "print(f\"  Input Vocab Size: {vocab_size}\")\n",
        "print(f\"  Output Embedding Dim (d_model): {d_model}\")\n",
        "print(f\"  Weight shape: {token_embedding_table.weight.shape}\")\n",
        "print(f\"  Device: {token_embedding_table.weight.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-embed-output",
      "metadata": {
        "id": "llama4-init-embed-output"
      },
      "source": [
        "**Output Explanation:** Creates the `nn.Embedding` layer, specifying the vocabulary size and the desired embedding dimension (`d_model`). It's moved to the target `device`. The output confirms the layer's configuration and the shape of its learnable weight matrix (`vocab_size` x `d_model`)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-rope-theory",
      "metadata": {
        "id": "llama4-init-rope-theory"
      },
      "source": [
        "#### Step 4.2: Rotary Positional Embedding (RoPE) Precomputation\n",
        "\n",
        "**Theory:** RoPE injects positional information by rotating pairs of features in the query (Q) and key (K) vectors based on their absolute position. It uses complex numbers conceptually. We precompute the inverse frequencies (`inv_freq`) based on `rope_theta` and the dimension of each attention head (`d_k`). The actual rotation angles (`freqs_cis`) depend on the token positions and are calculated dynamically within the forward pass.\n",
        "Formulas:\n",
        "$$ \\theta_i = \\frac{1}{\\text{rope_theta}^{2i / d_k}} $$\n",
        "where $i \\in [0, 1, ..., d_k/2 - 1]$. We precompute `inv_freq` which corresponds to this $\\theta_i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "llama4-init-rope-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-init-rope-code",
        "outputId": "0b2d074f-30f3-4ca4-91d6-aac63c4382a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precomputed RoPE inverse frequencies (inv_freq):\n",
            "  Shape: torch.Size([16])\n",
            "  Values (first 5): [1.0, 0.5623413324356079, 0.3162277638912201, 0.17782793939113617, 0.10000000149011612]\n",
            "  Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Precompute the inverse frequencies for RoPE\n",
        "# Formula: 1.0 / (rope_theta ** (torch.arange(0, d_k, 2) / d_k))\n",
        "rope_freq_indices = torch.arange(0, d_k, 2, dtype=torch.float, device=device)\n",
        "inv_freq = 1.0 / (rope_theta ** (rope_freq_indices / d_k))\n",
        "\n",
        "print(\"Precomputed RoPE inverse frequencies (inv_freq):\")\n",
        "print(f\"  Shape: {inv_freq.shape}\") # Should be (d_k / 2,)\n",
        "print(f\"  Values (first 5): {inv_freq[:5].tolist()}\")\n",
        "print(f\"  Device: {inv_freq.device}\")\n",
        "# The 'freqs_cis' (complex numbers) will be computed in the forward pass using these inv_freq and position_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-rope-output",
      "metadata": {
        "id": "llama4-init-rope-output"
      },
      "source": [
        "**Output Explanation:** This block calculates the `inv_freq` tensor based on the `rope_theta` and `d_k` hyperparameters. This tensor contains the fundamental frequencies used for RoPE. Its shape is `(d_k / 2,)`. The actual rotation values (`freqs_cis`) will be computed dynamically during the forward pass based on token positions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-rmsnorm-theory",
      "metadata": {
        "id": "llama4-init-rmsnorm-theory"
      },
      "source": [
        "#### Step 4.3: RMSNorm Layers Initialization\n",
        "\n",
        "**Theory:** RMSNorm is a simplification of Layer Normalization. It normalizes the input by its root mean square and then scales it by a learnable weight parameter `gamma` (but typically lacks the learnable bias `beta` of LayerNorm).\n",
        "Formula: $$ \\text{RMSNorm}(x) = \\frac{x}{\\sqrt{\\text{mean}(x^2) + \\epsilon}} * \\gamma $$\n",
        "We need RMSNorm layers before the attention block, before the MoE/FFN block, and one final RMSNorm before the output layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "llama4-init-rmsnorm-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-init-rmsnorm-code",
        "outputId": "db44ed10-cb18-4a00-8f55-64c9ab644422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing RMSNorm weights for 4 layers...\n",
            "  Initialized RMSNorm weights for Layer 1 (Input: torch.Size([128]), PostAttn: torch.Size([128]))\n",
            "  Initialized RMSNorm weights for Layer 2 (Input: torch.Size([128]), PostAttn: torch.Size([128]))\n",
            "  Initialized RMSNorm weights for Layer 3 (Input: torch.Size([128]), PostAttn: torch.Size([128]))\n",
            "  Initialized RMSNorm weights for Layer 4 (Input: torch.Size([128]), PostAttn: torch.Size([128]))\n",
            "Initialized Final RMSNorm weight, shape: torch.Size([128])\n",
            "RMSNorm weights initialized (as nn.Parameter). The normalization logic will be inline.\n"
          ]
        }
      ],
      "source": [
        "# Lists to store RMSNorm layer weights for each Transformer block\n",
        "rmsnorm_weights_input = []      # RMSNorm before MHA\n",
        "rmsnorm_weights_post_attn = []  # RMSNorm before MoE/FFN\n",
        "\n",
        "print(f\"Initializing RMSNorm weights for {n_layers} layers...\")\n",
        "for i in range(n_layers):\n",
        "    # RMSNorm weight for input to attention\n",
        "    # Initialize weight as torch.ones, similar to nn.LayerNorm's default gamma\n",
        "    weight_in = nn.Parameter(torch.ones(d_model, device=device))\n",
        "    rmsnorm_weights_input.append(weight_in)\n",
        "\n",
        "    # RMSNorm weight for input to MoE/FFN (post-attention)\n",
        "    weight_post = nn.Parameter(torch.ones(d_model, device=device))\n",
        "    rmsnorm_weights_post_attn.append(weight_post)\n",
        "    print(f\"  Initialized RMSNorm weights for Layer {i+1} (Input: {weight_in.shape}, PostAttn: {weight_post.shape})\")\n",
        "\n",
        "# Final RMSNorm before the output layer\n",
        "final_rmsnorm_weight = nn.Parameter(torch.ones(d_model, device=device))\n",
        "\n",
        "print(f\"Initialized Final RMSNorm weight, shape: {final_rmsnorm_weight.shape}\")\n",
        "print(\"RMSNorm weights initialized (as nn.Parameter). The normalization logic will be inline.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-rmsnorm-output",
      "metadata": {
        "id": "llama4-init-rmsnorm-output"
      },
      "source": [
        "**Output Explanation:** Creates the learnable weight parameters (`gamma`) for each RMSNorm instance needed in the model (two per layer, plus one final). These are stored as `nn.Parameter` objects containing tensors initialized to ones, with shape `(d_model,)`. The actual normalization calculation will be performed inline during the forward pass using these weights."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-attn-theory",
      "metadata": {
        "id": "llama4-init-attn-theory"
      },
      "source": [
        "#### Step 4.4: Attention Layers Initialization (MHA)\n",
        "\n",
        "**Theory:** Initialize the linear projection layers needed for Multi-Head Attention within each Transformer block. We need:\n",
        "1.  A combined projection layer to generate Query (Q), Key (K), and Value (V) vectors for all heads simultaneously. Input `(B, T, C)` -> Output `(B, T, 3*C)`.\n",
        "2.  An output projection layer to combine the results from all heads back into the model dimension. Input `(B, T, C)` -> Output `(B, T, C)`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "llama4-init-attn-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-init-attn-code",
        "outputId": "045d60c5-ce1e-43a8-b43e-dde198841868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Attention (MHA) linear layers for 4 layers...\n",
            "  Initialized MHA Linears for Layer 1 (QKV: torch.Size([384, 128]), Out: torch.Size([128, 128]))\n",
            "  Initialized MHA Linears for Layer 2 (QKV: torch.Size([384, 128]), Out: torch.Size([128, 128]))\n",
            "  Initialized MHA Linears for Layer 3 (QKV: torch.Size([384, 128]), Out: torch.Size([128, 128]))\n",
            "  Initialized MHA Linears for Layer 4 (QKV: torch.Size([384, 128]), Out: torch.Size([128, 128]))\n",
            "Attention (MHA) linear layers initialized.\n"
          ]
        }
      ],
      "source": [
        "# Lists to store Attention layers for each Transformer block\n",
        "mha_qkv_linears = []    # Combined Linear layer for Q, K, V projections\n",
        "mha_output_linears = [] # Output Linear layer for MHA\n",
        "\n",
        "print(f\"Initializing Attention (MHA) linear layers for {n_layers} layers...\")\n",
        "for i in range(n_layers):\n",
        "    # Combined QKV projection layer\n",
        "    # Bias is often False in large transformer QKV projections\n",
        "    qkv_linear = nn.Linear(d_model, 3 * d_model, bias=False).to(device)\n",
        "    mha_qkv_linears.append(qkv_linear)\n",
        "\n",
        "    # Output projection layer\n",
        "    # Bias is often False here too, but can be True\n",
        "    output_linear = nn.Linear(d_model, d_model, bias=False).to(device)\n",
        "    mha_output_linears.append(output_linear)\n",
        "    print(f\"  Initialized MHA Linears for Layer {i+1} (QKV: {qkv_linear.weight.shape}, Out: {output_linear.weight.shape})\")\n",
        "\n",
        "print(\"Attention (MHA) linear layers initialized.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-attn-output",
      "metadata": {
        "id": "llama4-init-attn-output"
      },
      "source": [
        "**Output Explanation:** Creates the `nn.Linear` layers responsible for projecting the input into Q, K, V spaces and projecting the attention output back to `d_model` for each of the `n_layers`. The output confirms the creation and shows the shapes of the weight matrices for these linear layers."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-moe-theory",
      "metadata": {
        "id": "llama4-init-moe-theory"
      },
      "source": [
        "#### Step 4.5: Mixture-of-Experts (MoE) Layers Initialization\n",
        "\n",
        "**Theory:** Initialize the components for the MoE blocks, which replace the standard FFN in each Transformer layer. For each layer, we need:\n",
        "1.  **Router:** A linear layer mapping the input hidden state (`d_model`) to logits for each expert (`num_local_experts`).\n",
        "2.  **Experts:** A set of `num_local_experts` independent MLP networks. Each expert MLP typically uses a gated activation (SiLU). We initialize the weights for these expert MLPs.\n",
        "    *   Gate/Up Projection: Linear `d_model` -> `2 * expert_dim`.\n",
        "    *   Down Projection: Linear `expert_dim` -> `d_model`.\n",
        "3.  **Shared Expert:** A single standard MLP (identical structure to one expert, but using `shared_expert_dim`) that processes all tokens.\n",
        "\n",
        "*Note:* We initialize the weights directly as `nn.Parameter` for the experts to closely follow the reference script's structure, rather than using `nn.Linear`. The shared expert uses standard `nn.Linear`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "llama4-init-moe-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-init-moe-code",
        "outputId": "44235f4b-1ef9-410b-d87a-6e6c9255851e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing MoE and Shared MLP components for 4 layers...\n",
            "  Num Experts per layer: 4\n",
            "  Expert Dim: 256\n",
            "  Shared MLP Dim: 256\n",
            "  Initialized MoE components for Layer 1:\n",
            "    Router weights: torch.Size([4, 128])\n",
            "    Expert Gate/Up weights: torch.Size([4, 128, 512])\n",
            "    Expert Down weights: torch.Size([4, 256, 128])\n",
            "    Shared Gate weights: torch.Size([256, 128])\n",
            "    Shared Up weights: torch.Size([256, 128])\n",
            "    Shared Down weights: torch.Size([128, 256])\n",
            "  Initialized MoE components for Layer 2:\n",
            "    Router weights: torch.Size([4, 128])\n",
            "    Expert Gate/Up weights: torch.Size([4, 128, 512])\n",
            "    Expert Down weights: torch.Size([4, 256, 128])\n",
            "    Shared Gate weights: torch.Size([256, 128])\n",
            "    Shared Up weights: torch.Size([256, 128])\n",
            "    Shared Down weights: torch.Size([128, 256])\n",
            "  Initialized MoE components for Layer 3:\n",
            "    Router weights: torch.Size([4, 128])\n",
            "    Expert Gate/Up weights: torch.Size([4, 128, 512])\n",
            "    Expert Down weights: torch.Size([4, 256, 128])\n",
            "    Shared Gate weights: torch.Size([256, 128])\n",
            "    Shared Up weights: torch.Size([256, 128])\n",
            "    Shared Down weights: torch.Size([128, 256])\n",
            "  Initialized MoE components for Layer 4:\n",
            "    Router weights: torch.Size([4, 128])\n",
            "    Expert Gate/Up weights: torch.Size([4, 128, 512])\n",
            "    Expert Down weights: torch.Size([4, 256, 128])\n",
            "    Shared Gate weights: torch.Size([256, 128])\n",
            "    Shared Up weights: torch.Size([256, 128])\n",
            "    Shared Down weights: torch.Size([128, 256])\n",
            "MoE and Shared MLP components initialized.\n"
          ]
        }
      ],
      "source": [
        "# Lists to store MoE components for each layer\n",
        "moe_routers = []             # Router linear layers\n",
        "moe_expert_gate_up_proj = [] # Expert Gate/Up projection weights\n",
        "moe_expert_down_proj = []    # Expert Down projection weights\n",
        "shared_expert_gate_proj = [] # Shared Expert Gate projection\n",
        "shared_expert_up_proj = []   # Shared Expert Up projection\n",
        "shared_expert_down_proj = [] # Shared Expert Down projection\n",
        "\n",
        "print(f\"Initializing MoE and Shared MLP components for {n_layers} layers...\")\n",
        "print(f\"  Num Experts per layer: {num_local_experts}\")\n",
        "print(f\"  Expert Dim: {expert_dim}\")\n",
        "print(f\"  Shared MLP Dim: {shared_expert_dim}\")\n",
        "\n",
        "for i in range(n_layers):\n",
        "    # 1. Router\n",
        "    router_linear = nn.Linear(d_model, num_local_experts, bias=False).to(device)\n",
        "    moe_routers.append(router_linear)\n",
        "\n",
        "    # 2. Experts (Weights as Parameters)\n",
        "    # Gate/Up Projection Weight: (num_experts, d_model, 2 * expert_dim)\n",
        "    gate_up_w = nn.Parameter(torch.empty(num_local_experts, d_model, 2 * expert_dim, device=device))\n",
        "    nn.init.normal_(gate_up_w, mean=0.0, std=0.02) # Example initialization\n",
        "    moe_expert_gate_up_proj.append(gate_up_w)\n",
        "\n",
        "    # Down Projection Weight: (num_experts, expert_dim, d_model)\n",
        "    down_w = nn.Parameter(torch.empty(num_local_experts, expert_dim, d_model, device=device))\n",
        "    nn.init.normal_(down_w, mean=0.0, std=0.02) # Example initialization\n",
        "    moe_expert_down_proj.append(down_w)\n",
        "\n",
        "    # 3. Shared Expert (Standard MLP layers)\n",
        "    shared_gate = nn.Linear(d_model, shared_expert_dim, bias=False).to(device)\n",
        "    shared_up = nn.Linear(d_model, shared_expert_dim, bias=False).to(device)\n",
        "    shared_down = nn.Linear(shared_expert_dim, d_model, bias=False).to(device)\n",
        "    shared_expert_gate_proj.append(shared_gate)\n",
        "    shared_expert_up_proj.append(shared_up)\n",
        "    shared_expert_down_proj.append(shared_down)\n",
        "\n",
        "    print(f\"  Initialized MoE components for Layer {i+1}:\")\n",
        "    print(f\"    Router weights: {router_linear.weight.shape}\")\n",
        "    print(f\"    Expert Gate/Up weights: {gate_up_w.shape}\")\n",
        "    print(f\"    Expert Down weights: {down_w.shape}\")\n",
        "    print(f\"    Shared Gate weights: {shared_gate.weight.shape}\")\n",
        "    print(f\"    Shared Up weights: {shared_up.weight.shape}\")\n",
        "    print(f\"    Shared Down weights: {shared_down.weight.shape}\")\n",
        "\n",
        "print(\"MoE and Shared MLP components initialized.\")\n",
        "# Activation function (used inline)\n",
        "activation_fn = nn.SiLU()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-moe-output",
      "metadata": {
        "id": "llama4-init-moe-output"
      },
      "source": [
        "**Output Explanation:** This block initializes the MoE components for each layer. For each layer, it creates:\n",
        "*   A linear `router` layer.\n",
        "*   The weight `Parameter` tensors for the `gate_up_proj` and `down_proj` across all `num_local_experts`.\n",
        "*   The `nn.Linear` layers for the `shared_expert` MLP.\n",
        "The output confirms the initialization and shows the shapes of the weight tensors for each component in each layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-output-theory",
      "metadata": {
        "id": "llama4-init-output-theory"
      },
      "source": [
        "#### Step 4.6: Final Output Layer Initialization\n",
        "\n",
        "**Theory:** This final linear layer maps the output of the last Transformer block (after the final RMSNorm) from the model dimension `d_model` back to the vocabulary size `vocab_size`. The output represents the raw scores (logits) for each possible next token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "llama4-init-output-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-init-output-code",
        "outputId": "bbdc82de-ba8c-4a39-b137-e3c3203123ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized Final Output Linear Layer:\n",
            "  Input Dim (d_model): 128\n",
            "  Output Dim (vocab_size): 36\n",
            "  Weight shape: torch.Size([36, 128])\n",
            "  Device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Final Linear Layer (language modeling head)\n",
        "output_linear_layer = nn.Linear(d_model, vocab_size, bias=False).to(device)\n",
        "\n",
        "print(f\"Initialized Final Output Linear Layer:\")\n",
        "print(f\"  Input Dim (d_model): {d_model}\")\n",
        "print(f\"  Output Dim (vocab_size): {vocab_size}\")\n",
        "print(f\"  Weight shape: {output_linear_layer.weight.shape}\")\n",
        "print(f\"  Device: {output_linear_layer.weight.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-output-output",
      "metadata": {
        "id": "llama4-init-output-output"
      },
      "source": [
        "**Output Explanation:** Initializes the final `nn.Linear` layer responsible for producing the logits over the vocabulary. The output confirms its input/output dimensions and weight shape."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-mask-theory",
      "metadata": {
        "id": "llama4-init-mask-theory"
      },
      "source": [
        "#### Step 4.7: Causal Mask Precomputation\n",
        "\n",
        "**Theory:** For decoder-only language models, we need a causal attention mask to prevent attention heads from looking at future tokens during training and generation. This is typically a lower triangular matrix where positions allowed to attend have a value (e.g., 1 or 0) and positions not allowed have another value (e.g., 0 or -infinity). We precompute this mask for the maximum `block_size`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "llama4-init-mask-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-init-mask-code",
        "outputId": "4a56a78c-da76-4f86-8a88-28af4ee0eb7a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precomputed Causal Attention Mask:\n",
            "  Shape: torch.Size([1, 1, 64, 64])\n",
            "  Requires grad: False\n"
          ]
        }
      ],
      "source": [
        "# Create the lower triangular mask for causal self-attention\n",
        "# Values are 1 where attention is allowed, 0 where it's masked.\n",
        "# Shape: (1, 1, block_size, block_size) for broadcasting with (B, n_heads, T, T)\n",
        "causal_mask = torch.tril(torch.ones(block_size, block_size, device=device))\n",
        "causal_mask = causal_mask.view(1, 1, block_size, block_size)\n",
        "\n",
        "print(\"Precomputed Causal Attention Mask:\")\n",
        "print(f\"  Shape: {causal_mask.shape}\")\n",
        "print(f\"  Requires grad: {causal_mask.requires_grad}\")\n",
        "# Optional: Visualize the mask for a smaller block size\n",
        "# if block_size <= 8:\n",
        "#    print(causal_mask[0, 0].cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-init-mask-output",
      "metadata": {
        "id": "llama4-init-mask-output"
      },
      "source": [
        "**Output Explanation:** Creates a lower triangular matrix representing the causal mask. The shape `(1, 1, block_size, block_size)` is suitable for broadcasting during the attention calculation. The output confirms the shape."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-trainsetup-theory",
      "metadata": {
        "id": "llama4-trainsetup-theory"
      },
      "source": [
        "### Step 5: Training Setup\n",
        "\n",
        "**Goal:** Prepare the optimizer and loss function needed for the training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-trainsetup-optim-theory",
      "metadata": {
        "id": "llama4-trainsetup-optim-theory"
      },
      "source": [
        "#### Step 5.1: Define Optimizer\n",
        "\n",
        "**Theory:** The optimizer is responsible for updating the model's learnable parameters based on the gradients calculated during backpropagation. We use AdamW, a common choice for Transformer models. We must gather *all* `nn.Parameter` objects from our initialized components into a list for the optimizer to manage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "llama4-trainsetup-optim-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-trainsetup-optim-code",
        "outputId": "2a7f070e-26c3-496a-8ab7-f0068ae9d2f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimizer Setup:\n",
            "  Optimizer: AdamW\n",
            "  Learning Rate: 0.0005\n",
            "  Managing 43 parameter groups/tensors.\n",
            "  Total Trainable Parameters: 2,240,640\n"
          ]
        }
      ],
      "source": [
        "# Gather all model parameters requiring gradients\n",
        "all_model_parameters = list(token_embedding_table.parameters())\n",
        "# Add RMSNorm weights\n",
        "all_model_parameters.extend(rmsnorm_weights_input)\n",
        "all_model_parameters.extend(rmsnorm_weights_post_attn)\n",
        "all_model_parameters.append(final_rmsnorm_weight)\n",
        "# Add Attention linear layer weights\n",
        "for i in range(n_layers):\n",
        "    all_model_parameters.extend(list(mha_qkv_linears[i].parameters()))\n",
        "    all_model_parameters.extend(list(mha_output_linears[i].parameters()))\n",
        "# Add MoE Router linear layer weights\n",
        "for i in range(n_layers):\n",
        "    all_model_parameters.extend(list(moe_routers[i].parameters()))\n",
        "# Add MoE Expert weights (already nn.Parameters)\n",
        "all_model_parameters.extend(moe_expert_gate_up_proj)\n",
        "all_model_parameters.extend(moe_expert_down_proj)\n",
        "# Add Shared Expert linear layer weights\n",
        "for i in range(n_layers):\n",
        "    all_model_parameters.extend(list(shared_expert_gate_proj[i].parameters()))\n",
        "    all_model_parameters.extend(list(shared_expert_up_proj[i].parameters()))\n",
        "    all_model_parameters.extend(list(shared_expert_down_proj[i].parameters()))\n",
        "# Add Final Output linear layer weights\n",
        "all_model_parameters.extend(list(output_linear_layer.parameters()))\n",
        "\n",
        "# Count total number of parameter tensors (groups)\n",
        "num_param_groups = len(all_model_parameters)\n",
        "# Count total number of individual parameters\n",
        "total_params = sum(p.numel() for p in all_model_parameters if p.requires_grad)\n",
        "\n",
        "# Define the AdamW optimizer\n",
        "optimizer = optim.AdamW(all_model_parameters, lr=learning_rate)\n",
        "\n",
        "print(\"Optimizer Setup:\")\n",
        "print(f\"  Optimizer: {type(optimizer).__name__}\")\n",
        "print(f\"  Learning Rate: {learning_rate}\")\n",
        "print(f\"  Managing {num_param_groups} parameter groups/tensors.\")\n",
        "print(f\"  Total Trainable Parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-trainsetup-optim-output",
      "metadata": {
        "id": "llama4-trainsetup-optim-output"
      },
      "source": [
        "**Output Explanation:** The code collects all learnable parameters (weights from Embeddings, RMSNorms, Linears, and the Expert parameters) into a single list. It then initializes the AdamW optimizer with this list and the specified learning rate. The output confirms the optimizer type, learning rate, the number of parameter tensors being managed, and the total count of individual trainable parameters in the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-trainsetup-loss-theory",
      "metadata": {
        "id": "llama4-trainsetup-loss-theory"
      },
      "source": [
        "#### Step 5.2: Define Loss Function\n",
        "\n",
        "**Theory:** For next-token prediction (a multi-class classification problem), the standard loss function is Cross-Entropy Loss. It compares the model's output logits with the true next-token IDs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "llama4-trainsetup-loss-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-trainsetup-loss-code",
        "outputId": "fd1760a4-bde1-44cd-a5c4-e7acdf74513c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss function defined: CrossEntropyLoss\n"
          ]
        }
      ],
      "source": [
        "# Define the loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(f\"Loss function defined: {type(criterion).__name__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-trainsetup-loss-output",
      "metadata": {
        "id": "llama4-trainsetup-loss-output"
      },
      "source": [
        "**Output Explanation:** Initializes the standard `nn.CrossEntropyLoss` function, which will be used to calculate the error between the model's predictions and the target sequences during training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-trainloop-theory",
      "metadata": {
        "id": "llama4-trainloop-theory"
      },
      "source": [
        "### Step 6: Training the Model (Inline Loop)\n",
        "\n",
        "**Goal:** Iteratively train the model by feeding it batches of data, calculating the loss, and updating the parameters using the optimizer. This is where all the previously initialized components come together in the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "llama4-trainloop-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "llama4-trainloop-code",
        "outputId": "496473c1-199e-4a88-d42d-9079c182fb8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Training Loop for 3000 epochs ---\n",
            "  Epoch 1/3000, Loss: 3.8124\n",
            "  Epoch 301/3000, Loss: 0.0734\n",
            "  Epoch 601/3000, Loss: 0.0595\n",
            "  Epoch 901/3000, Loss: 0.0609\n",
            "  Epoch 1201/3000, Loss: 0.0707\n",
            "  Epoch 1501/3000, Loss: 0.0664\n",
            "  Epoch 1801/3000, Loss: 0.0559\n",
            "  Epoch 2101/3000, Loss: 0.0610\n",
            "  Epoch 2401/3000, Loss: 0.0680\n",
            "  Epoch 2701/3000, Loss: 0.0641\n",
            "  Epoch 3000/3000, Loss: 0.0553\n",
            "--- Training Loop Completed ---\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "print(f\"\\n--- Starting Training Loop for {epochs} epochs ---\")\n",
        "\n",
        "# List to track losses\n",
        "losses = []\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    # --- 1. Batch Selection ---\n",
        "    # Select random indices for the batch\n",
        "    indices = torch.randint(0, num_sequences_available, (batch_size,))\n",
        "    # Get input and target sequences and move to device\n",
        "    xb = train_x[indices].to(device) # Input batch shape: (B, T)\n",
        "    yb = train_y[indices].to(device) # Target batch shape: (B, T)\n",
        "\n",
        "    # --- 2. Forward Pass (Inline execution) ---\n",
        "    B, T = xb.shape # B = batch_size, T = block_size\n",
        "    C = d_model     # Embedding dimension\n",
        "\n",
        "    # --- Initial Embedding ---\n",
        "    token_embed = token_embedding_table(xb) # (B, T, C)\n",
        "\n",
        "    # --- Prepare RoPE Frequencies for this batch/sequence length ---\n",
        "    # Create position IDs (0 to T-1)\n",
        "    position_ids = torch.arange(T, device=device).unsqueeze(0) # Shape: (1, T)\n",
        "    # Expand inv_freq for batch and sequence length\n",
        "    # inv_freq shape: (d_k/2) -> (1, d_k/2, 1) -> (B, d_k/2, 1)\n",
        "    inv_freq_expanded = inv_freq.unsqueeze(0).unsqueeze(-1).expand(B, -1, 1)\n",
        "    # position_ids shape: (1, T) -> (B, 1, T)\n",
        "    pos_ids_expanded = position_ids.expand(B, -1).unsqueeze(1).float()\n",
        "    # Calculate frequencies: (B, d_k/2, 1) @ (B, 1, T) -> (B, d_k/2, T)\n",
        "    with torch.autocast(device_type=device, enabled=False): # RoPE often done in float32\n",
        "        freqs = (inv_freq_expanded.float() @ pos_ids_expanded).transpose(1, 2) # (B, T, d_k/2)\n",
        "        # Convert to complex numbers (cis format: cos + i*sin)\n",
        "        freqs_cis = torch.polar(torch.ones_like(freqs), freqs) # (B, T, d_k/2)\n",
        "\n",
        "    # We don't add positional encoding directly; RoPE is applied in attention\n",
        "    x = token_embed\n",
        "\n",
        "    # --- Transformer Blocks Loop ---\n",
        "    for i in range(n_layers):\n",
        "        # Residual connection starts here\n",
        "        residual_attn = x\n",
        "\n",
        "        # --- Input RMSNorm ---\n",
        "        # Formula: x * rsqrt(mean(x^2) + eps) * gamma\n",
        "        x_float = x.float()\n",
        "        norm_const = torch.rsqrt(x_float.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
        "        x_norm = (x_float * norm_const).type_as(x)\n",
        "        x_norm = x_norm * rmsnorm_weights_input[i]\n",
        "\n",
        "        # --- Multi-Head Attention (MHA) ---\n",
        "        # QKV projection\n",
        "        qkv = mha_qkv_linears[i](x_norm) # (B, T, 3*C)\n",
        "        # Split heads\n",
        "        qkv = qkv.view(B, T, n_heads, 3 * d_k) # Reshape before permute\n",
        "        # Split Q, K, V\n",
        "        q, k, v = qkv.chunk(3, dim=-1) # (B, T, n_heads, d_k)\n",
        "\n",
        "        # --- Apply RoPE ---\n",
        "        # Inline apply_rotary_emb logic\n",
        "        # Reshape Q, K for complex multiplication: (B, T, n_heads, d_k/2, 2)\n",
        "        q_rope = q.float().reshape(B, T, n_heads, -1, 2)\n",
        "        k_rope = k.float().reshape(B, T, n_heads, -1, 2)\n",
        "        # View as complex: (B, T, n_heads, d_k/2)\n",
        "        q_complex = torch.view_as_complex(q_rope)\n",
        "        k_complex = torch.view_as_complex(k_rope)\n",
        "        # Reshape freqs_cis for broadcasting: (B, T, 1, d_k/2)\n",
        "        freqs_cis_bthd = freqs_cis.unsqueeze(2)\n",
        "        # Apply rotation\n",
        "        q_rotated = q_complex * freqs_cis_bthd\n",
        "        k_rotated = k_complex * freqs_cis_bthd\n",
        "        # Convert back to real: (B, T, n_heads, d_k/2, 2)\n",
        "        q_out_real = torch.view_as_real(q_rotated)\n",
        "        k_out_real = torch.view_as_real(k_rotated)\n",
        "        # Flatten last two dimensions: (B, T, n_heads, d_k)\n",
        "        q = q_out_real.flatten(3).type_as(q)\n",
        "        k = k_out_real.flatten(3).type_as(k)\n",
        "\n",
        "        # Permute for attention calculation: (B, n_heads, T, d_k)\n",
        "        q = q.permute(0, 2, 1, 3)\n",
        "        k = k.permute(0, 2, 1, 3)\n",
        "        v = v.permute(0, 2, 1, 3) # Permute v as well\n",
        "\n",
        "        # Scaled dot-product attention\n",
        "        # (B, n_heads, T, d_k) @ (B, n_heads, d_k, T) -> (B, n_heads, T, T)\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) * (d_k ** -0.5)\n",
        "        # Apply Causal Mask\n",
        "        attn_scores = attn_scores.masked_fill(causal_mask[:,:,:T,:T] == 0, float('-inf'))\n",
        "        attention_weights = F.softmax(attn_scores, dim=-1) # (B, n_heads, T, T)\n",
        "        # Handle potential NaNs if a row in softmax is all -inf (e.g., first token)\n",
        "        attention_weights = torch.nan_to_num(attention_weights)\n",
        "\n",
        "        # Attention output\n",
        "        # (B, n_heads, T, T) @ (B, n_heads, T, d_k) -> (B, n_heads, T, d_k)\n",
        "        attn_output = attention_weights @ v\n",
        "        # Concatenate heads: -> (B, T, n_heads, d_k) -> (B, T, C)\n",
        "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(B, T, C)\n",
        "\n",
        "        # Output projection\n",
        "        attn_output = mha_output_linears[i](attn_output)\n",
        "\n",
        "        # --- Add Residual Connection (Attention) ---\n",
        "        x = residual_attn + attn_output\n",
        "\n",
        "        # --- MoE Block ---\n",
        "        # Residual connection starts here\n",
        "        residual_moe = x\n",
        "\n",
        "        # --- Post-Attention RMSNorm ---\n",
        "        x_float = x.float()\n",
        "        norm_const = torch.rsqrt(x_float.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
        "        x_norm = (x_float * norm_const).type_as(x)\n",
        "        x_norm = x_norm * rmsnorm_weights_post_attn[i]\n",
        "\n",
        "        # --- Router Logits ---\n",
        "        # Input x_norm: (B, T, C) -> Router Output: (B, T, num_experts)\n",
        "        router_logits = moe_routers[i](x_norm)\n",
        "\n",
        "        # --- Expert Selection (Top-K) ---\n",
        "        # Get top-k experts and their routing weights (logits)\n",
        "        routing_weights, selected_experts = torch.topk(router_logits, num_experts_per_tok, dim=-1)\n",
        "        # Apply sigmoid to make weights sum to <= k (treating experts independently)\n",
        "        # Or use Softmax if weights should sum to 1 across the k experts\n",
        "        # Reference script uses sigmoid: B, T, k\n",
        "        routing_weights = torch.sigmoid(routing_weights)\n",
        "\n",
        "        # --- Prepare for Expert Calculation ---\n",
        "        # Flatten B and T dimensions to treat each token independently\n",
        "        x_flat = x_norm.view(-1, C) # (B*T, C)\n",
        "        selected_experts_flat = selected_experts.view(-1) # (B*T*k)\n",
        "        routing_weights_flat = routing_weights.view(-1)   # (B*T*k)\n",
        "\n",
        "        # Create token indices corresponding to the selected experts\n",
        "        # token_idx goes from 0 to B*T-1\n",
        "        # expert_idx is the index of the expert selected (0 to num_experts-1)\n",
        "        token_idx = torch.arange(B * T, device=device).repeat_interleave(num_experts_per_tok) # (B*T*k)\n",
        "        expert_idx = selected_experts_flat # (B*T*k)\n",
        "\n",
        "        # Gather the hidden states for each token * expert combination\n",
        "        # Input x_flat: (B*T, C)\n",
        "        # Index token_idx: (B*T*k)\n",
        "        # Output expert_inputs: (B*T*k, C)\n",
        "        expert_inputs = x_flat[token_idx]\n",
        "\n",
        "        # --- Run Experts ---\n",
        "        # Get expert weights for the selected experts\n",
        "        # moe_expert_gate_up_proj[i]: (num_experts, C, 2*expert_dim)\n",
        "        # Index expert_idx: (B*T*k)\n",
        "        # Output gate_up_w_selected: (B*T*k, C, 2*expert_dim)\n",
        "        gate_up_w_selected = moe_expert_gate_up_proj[i][expert_idx]\n",
        "        # moe_expert_down_proj[i]: (num_experts, expert_dim, C)\n",
        "        # Output down_w_selected: (B*T*k, expert_dim, C)\n",
        "        down_w_selected = moe_expert_down_proj[i][expert_idx]\n",
        "\n",
        "        # Perform batched matrix multiplication (BMM)\n",
        "        # expert_inputs: (B*T*k, C) -> unsqueeze for BMM: (B*T*k, 1, C)\n",
        "        # gate_up_w_selected: (B*T*k, C, 2*expert_dim)\n",
        "        # Output gate_up_states: (B*T*k, 1, 2*expert_dim)\n",
        "        gate_up_states = torch.bmm(expert_inputs.unsqueeze(1), gate_up_w_selected)\n",
        "\n",
        "        # Split gate and up states\n",
        "        gate, up = gate_up_states.chunk(2, dim=-1) # Each: (B*T*k, 1, expert_dim)\n",
        "\n",
        "        # Apply activation function (SiLU) and gating\n",
        "        activated_states = activation_fn(gate) * up # (B*T*k, 1, expert_dim)\n",
        "\n",
        "        # Down projection\n",
        "        # activated_states: (B*T*k, 1, expert_dim)\n",
        "        # down_w_selected: (B*T*k, expert_dim, C)\n",
        "        # Output expert_outputs_flat: (B*T*k, 1, C) -> squeeze -> (B*T*k, C)\n",
        "        expert_outputs_flat = torch.bmm(activated_states, down_w_selected).squeeze(1) # (B*T*k, C)\n",
        "\n",
        "        # Weight the outputs by the routing weights\n",
        "        # expert_outputs_flat: (B*T*k, C)\n",
        "        # routing_weights_flat: (B*T*k) -> unsqueeze -> (B*T*k, 1)\n",
        "        expert_outputs_weighted = expert_outputs_flat * routing_weights_flat.unsqueeze(-1) # (B*T*k, C)\n",
        "\n",
        "        # --- Combine Expert Outputs ---\n",
        "        # Sum the outputs for each token (across the k selected experts)\n",
        "        # Need to scatter-add the weighted outputs back to the original token positions\n",
        "        # Initialize output tensor: (B*T, C)\n",
        "        combined_expert_outputs = torch.zeros_like(x_flat) # (B*T, C)\n",
        "        # Scatter Add:\n",
        "        # combined_expert_outputs: tensor to add into\n",
        "        # dim=0: dimension along which to index\n",
        "        # index=token_idx.unsqueeze(-1).expand(-1, C): indices to scatter to (needs C dimension)\n",
        "        # src=expert_outputs_weighted: values to add\n",
        "        combined_expert_outputs.scatter_add_(0, token_idx.unsqueeze(-1).expand(-1, C), expert_outputs_weighted)\n",
        "\n",
        "        # --- Run Shared Expert ---\n",
        "        shared_gate_val = shared_expert_gate_proj[i](x_norm) # (B, T, shared_expert_dim)\n",
        "        shared_up_val = shared_expert_up_proj[i](x_norm)     # (B, T, shared_expert_dim)\n",
        "        shared_activated = activation_fn(shared_gate_val) * shared_up_val # (B, T, shared_expert_dim)\n",
        "        shared_output = shared_expert_down_proj[i](shared_activated)      # (B, T, C)\n",
        "\n",
        "        # --- Combine MoE and Shared Expert ---\n",
        "        # Reshape combined_expert_outputs back to (B, T, C)\n",
        "        moe_output = combined_expert_outputs.view(B, T, C)\n",
        "        # Add shared expert output\n",
        "        final_moe_output = moe_output + shared_output\n",
        "\n",
        "        # --- Add Residual Connection (MoE/FFN) ---\n",
        "        x = residual_moe + final_moe_output\n",
        "\n",
        "    # --- Final RMSNorm ---\n",
        "    x_float = x.float()\n",
        "    norm_const = torch.rsqrt(x_float.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
        "    x_norm = (x_float * norm_const).type_as(x)\n",
        "    x_norm = x_norm * final_rmsnorm_weight\n",
        "\n",
        "    # --- Final Output Layer ---\n",
        "    logits = output_linear_layer(x_norm) # (B, T, vocab_size)\n",
        "\n",
        "    # --- 3. Calculate Loss ---\n",
        "    B_loss, T_loss, V_loss = logits.shape\n",
        "    logits_for_loss = logits.view(B_loss * T_loss, V_loss)\n",
        "    targets_for_loss = yb.view(B_loss * T_loss)\n",
        "    loss = criterion(logits_for_loss, targets_for_loss)\n",
        "\n",
        "    # --- 4. Zero Gradients ---\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # --- 5. Backward Pass ---\n",
        "    loss.backward()\n",
        "\n",
        "    # --- 6. Update Parameters ---\n",
        "    optimizer.step()\n",
        "\n",
        "    # --- Logging ---\n",
        "    current_loss = loss.item()\n",
        "    losses.append(current_loss)\n",
        "    if epoch % eval_interval == 0 or epoch == epochs - 1:\n",
        "        print(f\"  Epoch {epoch+1}/{epochs}, Loss: {current_loss:.4f}\")\n",
        "\n",
        "print(\"--- Training Loop Completed ---\")\n",
        "\n",
        "# Optional: Plot losses\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.plot(losses)\n",
        "    plt.title(\"Training Loss Over Epochs\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "except ImportError:\n",
        "    print(\"Matplotlib not found, skipping loss plot.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-trainloop-output",
      "metadata": {
        "id": "llama4-trainloop-output"
      },
      "source": [
        "**Output Explanation:** This crucial block executes the training. For each epoch, it:\n",
        "1.  Selects a batch of input (`xb`) and target (`yb`) sequences.\n",
        "2.  Performs the full forward pass:\n",
        "    *   Embeds tokens.\n",
        "    *   Calculates RoPE frequencies (`freqs_cis`).\n",
        "    *   Iterates through `n_layers`:\n",
        "        *   Applies input RMSNorm.\n",
        "        *   Calculates Multi-Head Attention, including applying RoPE to queries and keys.\n",
        "        *   Adds the first residual connection.\n",
        "        *   Applies post-attention RMSNorm.\n",
        "        *   Executes the MoE logic: routing, expert selection (`topk`), weighted gathering, expert processing (gated MLP via BMM), shared expert processing, combination (`scatter_add_`), and addition.\n",
        "        *   Adds the second residual connection.\n",
        "    *   Applies final RMSNorm.\n",
        "    *   Projects to vocabulary logits.\n",
        "3.  Calculates the Cross-Entropy loss between the predicted logits and the actual target tokens.\n",
        "4.  Performs backpropagation to compute gradients.\n",
        "5.  Updates the model parameters using the optimizer.\n",
        "The output shows the loss value printed periodically, which should generally decrease over time, indicating that the model is learning."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-gen-theory",
      "metadata": {
        "id": "llama4-gen-theory"
      },
      "source": [
        "### Step 7: Text Generation (Inline)\n",
        "\n",
        "**Goal:** Use the trained model parameters to generate new text autoregressively, starting from a seed context."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-gen-setup-theory",
      "metadata": {
        "id": "llama4-gen-setup-theory"
      },
      "source": [
        "#### Step 7.1: Generation Setup\n",
        "\n",
        "**Theory:** Define the starting prompt (seed characters), convert it to token IDs, and specify how many new tokens to generate. Crucially, set all model components to evaluation mode (`.eval()`) - although less critical here as we didn't use dropout/batchnorm, it's vital practice. We also use `torch.no_grad()` to disable gradient calculation for efficiency during inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "llama4-gen-setup-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-gen-setup-code",
        "outputId": "304e048a-57fd-4697-b993-9f885fa35fc2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Step 7: Text Generation ---\n",
            "Seed text: 'Alice '\n",
            "Generating 200 new tokens...\n",
            "Initial context shape: torch.Size([1, 6])\n",
            "Model components set to evaluation mode (where applicable).\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n--- Step 7: Text Generation ---\")\n",
        "\n",
        "# --- Generation Parameters ---\n",
        "seed_chars = \"Alice \" # Starting text prompt\n",
        "num_tokens_to_generate = 200 # How many new characters to generate\n",
        "print(f\"Seed text: '{seed_chars}'\")\n",
        "print(f\"Generating {num_tokens_to_generate} new tokens...\")\n",
        "\n",
        "# --- Prepare Initial Context ---\n",
        "# Convert seed characters to token IDs\n",
        "seed_ids = [char_to_int[ch] for ch in seed_chars if ch in char_to_int]\n",
        "# Create the initial context tensor (add batch dimension)\n",
        "generated_sequence = torch.tensor([seed_ids], dtype=torch.long, device=device)\n",
        "print(f\"Initial context shape: {generated_sequence.shape}\")\n",
        "\n",
        "# --- Set Model Components to Evaluation Mode ---\n",
        "# (Important if Dropout or BatchNorm were used, good practice anyway)\n",
        "token_embedding_table.eval()\n",
        "for i in range(n_layers):\n",
        "    # RMSNorm doesn't have eval mode, just use weights\n",
        "    mha_qkv_linears[i].eval()\n",
        "    mha_output_linears[i].eval()\n",
        "    moe_routers[i].eval()\n",
        "    # Expert weights (Parameters) don't have eval()\n",
        "    shared_expert_gate_proj[i].eval()\n",
        "    shared_expert_up_proj[i].eval()\n",
        "    shared_expert_down_proj[i].eval()\n",
        "output_linear_layer.eval()\n",
        "# Final RMSNorm weight doesn't have eval()\n",
        "print(\"Model components set to evaluation mode (where applicable).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-gen-setup-output",
      "metadata": {
        "id": "llama4-gen-setup-output"
      },
      "source": [
        "**Output Explanation:** Sets the seed text and the desired generation length. It converts the seed text into token IDs and prepares the initial context tensor. It also explicitly sets the relevant `nn.Module` components (like Linear layers) to evaluation mode."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-gen-loop-theory",
      "metadata": {
        "id": "llama4-gen-loop-theory"
      },
      "source": [
        "#### Step 7.2: The Generation Loop\n",
        "\n",
        "**Theory:** Generate text token by token:\n",
        "1.  Take the current generated sequence (or the last `block_size` tokens of it) as context.\n",
        "2.  Perform a forward pass through the model (using the same inline logic as training) to get logits for the *next* token.\n",
        "3.  Convert the logits for the very last position into probabilities using Softmax.\n",
        "4.  Sample the next token ID from this probability distribution (using `torch.multinomial`).\n",
        "5.  Append the sampled token ID to the sequence.\n",
        "6.  Repeat until the desired number of tokens is generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "llama4-gen-loop-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-gen-loop-code",
        "outputId": "7fc801d9-9a1f-4775-c57f-bac393441477"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting generation loop...\n",
            "...Generation loop finished.\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting generation loop...\")\n",
        "# Disable gradient calculations\n",
        "with torch.no_grad():\n",
        "    for _ in range(num_tokens_to_generate):\n",
        "        # --- 1. Prepare Input Context ---\n",
        "        # Ensure context doesn't exceed block_size\n",
        "        current_context = generated_sequence[:, -block_size:]\n",
        "        B_gen, T_gen = current_context.shape\n",
        "        C_gen = d_model\n",
        "\n",
        "        # --- 2. Forward Pass (Identical logic to training loop's forward pass) ---\n",
        "        # --- Initial Embedding ---\n",
        "        token_embed_gen = token_embedding_table(current_context) # (B_gen, T_gen, C_gen)\n",
        "        # --- Prepare RoPE Frequencies ---\n",
        "        position_ids_gen = torch.arange(T_gen, device=device).unsqueeze(0) # (1, T_gen)\n",
        "        inv_freq_expanded_gen = inv_freq.unsqueeze(0).unsqueeze(-1).expand(B_gen, -1, 1)\n",
        "        pos_ids_expanded_gen = position_ids_gen.expand(B_gen, -1).unsqueeze(1).float()\n",
        "        with torch.autocast(device_type=device, enabled=False):\n",
        "            freqs_gen = (inv_freq_expanded_gen.float() @ pos_ids_expanded_gen).transpose(1, 2)\n",
        "            freqs_cis_gen = torch.polar(torch.ones_like(freqs_gen), freqs_gen)\n",
        "        x_gen = token_embed_gen\n",
        "\n",
        "        # --- Transformer Blocks Loop ---\n",
        "        for i in range(n_layers):\n",
        "            residual_attn_gen = x_gen\n",
        "            # --- Input RMSNorm ---\n",
        "            x_float_gen = x_gen.float()\n",
        "            norm_const_gen = torch.rsqrt(x_float_gen.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
        "            x_norm_gen = (x_float_gen * norm_const_gen).type_as(x_gen)\n",
        "            x_norm_gen = x_norm_gen * rmsnorm_weights_input[i]\n",
        "            # --- MHA ---\n",
        "            qkv_gen = mha_qkv_linears[i](x_norm_gen)\n",
        "            qkv_gen = qkv_gen.view(B_gen, T_gen, n_heads, 3 * d_k)\n",
        "            q_gen, k_gen, v_gen = qkv_gen.chunk(3, dim=-1)\n",
        "            # --- Apply RoPE ---\n",
        "            q_rope_gen = q_gen.float().reshape(B_gen, T_gen, n_heads, -1, 2)\n",
        "            k_rope_gen = k_gen.float().reshape(B_gen, T_gen, n_heads, -1, 2)\n",
        "            q_complex_gen = torch.view_as_complex(q_rope_gen)\n",
        "            k_complex_gen = torch.view_as_complex(k_rope_gen)\n",
        "            freqs_cis_bthd_gen = freqs_cis_gen.unsqueeze(2)\n",
        "            q_rotated_gen = q_complex_gen * freqs_cis_bthd_gen\n",
        "            k_rotated_gen = k_complex_gen * freqs_cis_bthd_gen\n",
        "            q_out_real_gen = torch.view_as_real(q_rotated_gen)\n",
        "            k_out_real_gen = torch.view_as_real(k_rotated_gen)\n",
        "            q_gen = q_out_real_gen.flatten(3).type_as(q_gen)\n",
        "            k_gen = k_out_real_gen.flatten(3).type_as(k_gen)\n",
        "            # --- Attention Calculation ---\n",
        "            q_gen = q_gen.permute(0, 2, 1, 3)\n",
        "            k_gen = k_gen.permute(0, 2, 1, 3)\n",
        "            v_gen = v_gen.permute(0, 2, 1, 3)\n",
        "            attn_scores_gen = (q_gen @ k_gen.transpose(-2, -1)) * (d_k ** -0.5)\n",
        "            # Use causal mask sliced to T_gen\n",
        "            attn_scores_gen = attn_scores_gen.masked_fill(causal_mask[:,:,:T_gen,:T_gen] == 0, float('-inf'))\n",
        "            attention_weights_gen = F.softmax(attn_scores_gen, dim=-1)\n",
        "            attention_weights_gen = torch.nan_to_num(attention_weights_gen)\n",
        "            attn_output_gen = attention_weights_gen @ v_gen\n",
        "            attn_output_gen = attn_output_gen.permute(0, 2, 1, 3).contiguous().view(B_gen, T_gen, C_gen)\n",
        "            attn_output_gen = mha_output_linears[i](attn_output_gen)\n",
        "            # --- Add Residual 1 ---\n",
        "            x_gen = residual_attn_gen + attn_output_gen\n",
        "            # --- MoE Block ---\n",
        "            residual_moe_gen = x_gen\n",
        "            # --- Post-Attention RMSNorm ---\n",
        "            x_float_gen = x_gen.float()\n",
        "            norm_const_gen = torch.rsqrt(x_float_gen.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
        "            x_norm_gen = (x_float_gen * norm_const_gen).type_as(x_gen)\n",
        "            x_norm_gen = x_norm_gen * rmsnorm_weights_post_attn[i]\n",
        "            # --- Router, Experts, Shared (Inline MoE logic from training loop) ---\n",
        "            router_logits_gen = moe_routers[i](x_norm_gen)\n",
        "            routing_weights_gen, selected_experts_gen = torch.topk(router_logits_gen, num_experts_per_tok, dim=-1)\n",
        "            routing_weights_gen = torch.sigmoid(routing_weights_gen)\n",
        "            x_flat_gen = x_norm_gen.view(-1, C_gen)\n",
        "            selected_experts_flat_gen = selected_experts_gen.view(-1)\n",
        "            routing_weights_flat_gen = routing_weights_gen.view(-1)\n",
        "            token_idx_gen = torch.arange(B_gen * T_gen, device=device).repeat_interleave(num_experts_per_tok)\n",
        "            expert_idx_gen = selected_experts_flat_gen\n",
        "            expert_inputs_gen = x_flat_gen[token_idx_gen]\n",
        "            gate_up_w_selected_gen = moe_expert_gate_up_proj[i][expert_idx_gen]\n",
        "            down_w_selected_gen = moe_expert_down_proj[i][expert_idx_gen]\n",
        "            gate_up_states_gen = torch.bmm(expert_inputs_gen.unsqueeze(1), gate_up_w_selected_gen)\n",
        "            gate_gen, up_gen = gate_up_states_gen.chunk(2, dim=-1)\n",
        "            activated_states_gen = activation_fn(gate_gen) * up_gen\n",
        "            expert_outputs_flat_gen = torch.bmm(activated_states_gen, down_w_selected_gen).squeeze(1)\n",
        "            expert_outputs_weighted_gen = expert_outputs_flat_gen * routing_weights_flat_gen.unsqueeze(-1)\n",
        "            combined_expert_outputs_gen = torch.zeros_like(x_flat_gen)\n",
        "            combined_expert_outputs_gen.scatter_add_(0, token_idx_gen.unsqueeze(-1).expand(-1, C_gen), expert_outputs_weighted_gen)\n",
        "            shared_gate_val_gen = shared_expert_gate_proj[i](x_norm_gen)\n",
        "            shared_up_val_gen = shared_expert_up_proj[i](x_norm_gen)\n",
        "            shared_activated_gen = activation_fn(shared_gate_val_gen) * shared_up_val_gen\n",
        "            shared_output_gen = shared_expert_down_proj[i](shared_activated_gen)\n",
        "            moe_output_gen = combined_expert_outputs_gen.view(B_gen, T_gen, C_gen)\n",
        "            final_moe_output_gen = moe_output_gen + shared_output_gen\n",
        "            # --- Add Residual 2 ---\n",
        "            x_gen = residual_moe_gen + final_moe_output_gen\n",
        "\n",
        "        # --- Final RMSNorm ---\n",
        "        x_float_gen = x_gen.float()\n",
        "        norm_const_gen = torch.rsqrt(x_float_gen.pow(2).mean(-1, keepdim=True) + rms_norm_eps)\n",
        "        x_norm_gen = (x_float_gen * norm_const_gen).type_as(x_gen)\n",
        "        x_norm_gen = x_norm_gen * final_rmsnorm_weight\n",
        "\n",
        "        # --- Final Output Layer ---\n",
        "        logits_gen = output_linear_layer(x_norm_gen) # (B_gen, T_gen, vocab_size)\n",
        "\n",
        "        # --- 3. Get Logits for Last Time Step ---\n",
        "        logits_last_token = logits_gen[:, -1, :] # Shape: (B_gen, vocab_size)\n",
        "\n",
        "        # --- 4. Apply Softmax ---\n",
        "        probs = F.softmax(logits_last_token, dim=-1) # Shape: (B_gen, vocab_size)\n",
        "\n",
        "        # --- 5. Sample Next Token ---\n",
        "        next_token = torch.multinomial(probs, num_samples=1) # Shape: (B_gen, 1)\n",
        "\n",
        "        # --- 6. Append Sampled Token ---\n",
        "        generated_sequence = torch.cat((generated_sequence, next_token), dim=1)\n",
        "\n",
        "print(\"...Generation loop finished.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-gen-loop-output",
      "metadata": {
        "id": "llama4-gen-loop-output"
      },
      "source": [
        "**Output Explanation:** This block performs the actual text generation. It iteratively feeds the current sequence into the model, gets the probability distribution for the next token, samples from it, and appends the result. The output confirms the loop has started and finished."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-gen-decode-theory",
      "metadata": {
        "id": "llama4-gen-decode-theory"
      },
      "source": [
        "#### Step 7.3: Decode Generated Sequence\n",
        "\n",
        "**Theory:** Convert the final sequence of generated token IDs (including the initial seed) back into a human-readable string using the `int_to_char` mapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "llama4-gen-decode-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-gen-decode-code",
        "outputId": "4b0309bc-8001-44f1-a9cc-332d6716132f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Final Generated Text ---\n",
            "Alice 'without pictures or\n",
            "conversation?'\n",
            "So she was considering in her own mind (as well as she could, for the\n",
            "hot day made her feel very sleepy and stupid), whether the pleasure\n",
            "of making a daisy-chain wo\n"
          ]
        }
      ],
      "source": [
        "# Get the generated sequence for the first (and only) batch item\n",
        "final_generated_ids = generated_sequence[0].tolist()\n",
        "\n",
        "# Decode the list of IDs back into a string\n",
        "decoded_text = ''.join([int_to_char.get(id_val, '[UNK]') for id_val in final_generated_ids])\n",
        "\n",
        "print(\"\\n--- Final Generated Text ---\")\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-gen-decode-output",
      "metadata": {
        "id": "llama4-gen-decode-output"
      },
      "source": [
        "**Output Explanation:** Takes the final tensor of generated token IDs, converts it back into a list of integers, and uses the `int_to_char` dictionary to map each ID back to its corresponding character. The resulting characters are joined to form the final generated text string, which is then printed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-save-theory",
      "metadata": {
        "id": "llama4-save-theory"
      },
      "source": [
        "### Step 8: Save Model State (Optional)\n",
        "\n",
        "**Goal:** Save the trained parameters of the model to a file for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "llama4-save-code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llama4-save-code",
        "outputId": "b19172c7-be7f-4046-9b53-5cf5df2bd9b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model state saved successfully to 'saved_models/llama4_moe_model.pt'\n"
          ]
        }
      ],
      "source": [
        "# Create a directory to store the model (if it doesn't exist)\n",
        "save_dir = 'saved_models'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "save_path = os.path.join(save_dir, 'llama4_moe_model.pt')\n",
        "\n",
        "# Create a state dictionary manually collecting all components\n",
        "model_state = {\n",
        "    # Configuration\n",
        "    'config': {\n",
        "        'vocab_size': vocab_size,\n",
        "        'd_model': d_model,\n",
        "        'n_layers': n_layers,\n",
        "        'n_heads': n_heads,\n",
        "        'block_size': block_size,\n",
        "        'rms_norm_eps': rms_norm_eps,\n",
        "        'rope_theta': rope_theta,\n",
        "        'num_local_experts': num_local_experts,\n",
        "        'num_experts_per_tok': num_experts_per_tok,\n",
        "        'intermediate_size_expert': intermediate_size_expert,\n",
        "        'intermediate_size_shared': intermediate_size_shared\n",
        "    },\n",
        "    # Tokenizer\n",
        "    'tokenizer': {\n",
        "        'char_to_int': char_to_int,\n",
        "        'int_to_char': int_to_char\n",
        "    },\n",
        "    # Model Parameters (State Dicts for nn.Modules, Tensors for nn.Parameters)\n",
        "    'token_embedding_table': token_embedding_table.state_dict(),\n",
        "    'rmsnorm_weights_input': rmsnorm_weights_input, # List of Parameters\n",
        "    'rmsnorm_weights_post_attn': rmsnorm_weights_post_attn, # List of Parameters\n",
        "    'final_rmsnorm_weight': final_rmsnorm_weight, # Parameter\n",
        "    'mha_qkv_linears': [l.state_dict() for l in mha_qkv_linears],\n",
        "    'mha_output_linears': [l.state_dict() for l in mha_output_linears],\n",
        "    'moe_routers': [r.state_dict() for r in moe_routers],\n",
        "    'moe_expert_gate_up_proj': moe_expert_gate_up_proj, # List of Parameters\n",
        "    'moe_expert_down_proj': moe_expert_down_proj, # List of Parameters\n",
        "    'shared_expert_gate_proj': [l.state_dict() for l in shared_expert_gate_proj],\n",
        "    'shared_expert_up_proj': [l.state_dict() for l in shared_expert_up_proj],\n",
        "    'shared_expert_down_proj': [l.state_dict() for l in shared_expert_down_proj],\n",
        "    'output_linear_layer': output_linear_layer.state_dict(),\n",
        "    # Note: RoPE inv_freq is not saved as it's derived from config\n",
        "}\n",
        "\n",
        "# Save the state dictionary\n",
        "torch.save(model_state, save_path)\n",
        "\n",
        "print(f\"Model state saved successfully to '{save_path}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-save-output",
      "metadata": {
        "id": "llama4-save-output"
      },
      "source": [
        "**Output Explanation:** This block gathers the configuration, tokenizer, and the state of all learnable parameters (weights from `nn.Linear`, `nn.Embedding`, and the explicit `nn.Parameter` tensors for RMSNorm and Experts) into a Python dictionary. It then uses `torch.save` to serialize this dictionary to a file (`.pt`). The output confirms that the model state has been saved."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "llama4-conclusion",
      "metadata": {
        "id": "llama4-conclusion"
      },
      "source": [
        "### Step 9: Conclusion\n",
        "\n",
        "This notebook provided an extremely detailed, step-by-step, inline implementation of a Transformer-based language model incorporating key architectural elements inspired by models like Llama 4, particularly **Mixture-of-Experts (MoE)**, **RMS Normalization**, and **Rotary Positional Embeddings (RoPE)**.\n",
        "\n",
        "We covered:\n",
        "1.  **Setup & Tokenization:** Basic environment setup and character-level tokenization.\n",
        "2.  **Hyperparameter Definition:** Setting up configuration values, scaled down from larger models.\n",
        "3.  **Data Preparation:** Creating input/target sequences for next-token prediction.\n",
        "4.  **Model Initialization (Inline):** Explicitly creating and initializing components like token embeddings, RMSNorm weights, attention linear layers, RoPE frequency bases, MoE routers, MoE expert weights, shared expert MLPs, and the final output layer.\n",
        "5.  **Training Loop (Inline):** Implementing the complete forward pass within the loop, demonstrating:\n",
        "    *   Application of RMSNorm.\n",
        "    *   Calculation and application of RoPE within the MHA block.\n",
        "    *   The MoE forward pass: routing, expert selection (Top-K), parallel expert computation (using BMM), combination of expert outputs (`scatter_add_`), and integration with a shared expert MLP.\n",
        "    *   Standard Transformer operations like residual connections and attention.\n",
        "    *   Loss calculation, backpropagation, and optimizer steps.\n",
        "6.  **Text Generation:** Implementing autoregressive sampling using the trained model components in evaluation mode.\n",
        "\n",
        "While highly verbose and simplified in scale and certain features (like using MHA instead of GQA), this inline approach aimed to demystify the core mechanics, especially the MoE layer's operation (routing, expert processing, combination) and its integration within a modern Transformer architecture incorporating RMSNorm and RoPE. Building upon this understanding would involve using more structured code (functions, classes), implementing further optimizations (GQA, etc.), and training on significantly larger datasets."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv-kg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
